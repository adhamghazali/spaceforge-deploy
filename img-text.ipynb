{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c24dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bad45a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install huggingface_hub\n",
    "!pip install diffusers==0.2.4\n",
    "!pip install transformers scipy ftfy\n",
    "!pip install \"ipywidgets>=7,<8\"\n",
    "!nvidia-smi\n",
    "!pip install git+https://github.com/openai/CLIP.git \n",
    "!pip install realesrgan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f3501e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac58774a",
   "metadata": {},
   "outputs": [],
   "source": [
    "1+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89651c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f119d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import autocast, nn\n",
    "from PIL import Image\n",
    "import random\n",
    "from transformers import CLIPTextModel, CLIPTokenizer\n",
    "from diffusers import AutoencoderKL, UNet2DConditionModel, PNDMScheduler\n",
    "from diffusers import LMSDiscreteScheduler\n",
    "from torch import autocast\n",
    "import argparse\n",
    "import cv2\n",
    "import glob\n",
    "import os\n",
    "from basicsr.archs.rrdbnet_arch import RRDBNet\n",
    "from realesrgan import RealESRGANer\n",
    "from realesrgan.archs.srvgg_arch import SRVGGNetCompact\n",
    "\n",
    "import clip\n",
    "import os\n",
    "import numpy as np\n",
    "import torch.nn.functional as nnf\n",
    "import sys\n",
    "from typing import Tuple, List, Union, Optional\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, AdamW, get_linear_schedule_with_warmup\n",
    "from tqdm import tqdm, trange\n",
    "import skimage.io as io\n",
    "\n",
    "import transformers\n",
    "\n",
    "\n",
    "torch_device = \"cuda:3\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "\n",
    "# 1. Load the autoencoder model which will be used to decode the latents into image space. \n",
    "vae = AutoencoderKL.from_pretrained(\"../models/spaceforge-diffusion/vae\")\n",
    "# 2. Load the tokenizer and text encoder to tokenize and encode the text. \n",
    "tokenizer = CLIPTokenizer.from_pretrained(\"../models/clip-vit-large-patch14/\")\n",
    "text_encoder = CLIPTextModel.from_pretrained(\"../models/clip-vit-large-patch14/\")\n",
    "\n",
    "# 3. The UNet model for generating the latents.\n",
    "unet = UNet2DConditionModel.from_pretrained(\"../models/spaceforge-diffusion/unet/\")\n",
    "\n",
    "vae = vae.to(torch_device)\n",
    "text_encoder = text_encoder.to(torch_device)\n",
    "unet = unet.to(torch_device)\n",
    "\n",
    "\n",
    "SRmodel = RRDBNet(num_in_ch=3, num_out_ch=3, num_feat=64, num_block=23, num_grow_ch=32, scale=4)\n",
    "netscale = 4\n",
    "model_path='../models/gans/RealESRGAN_x4plus.pth'\n",
    "upsampler = RealESRGANer(\n",
    "        scale=netscale,\n",
    "        model_path=model_path,\n",
    "        model=SRmodel,\n",
    "        tile=0,\n",
    "        tile_pad=10,\n",
    "        pre_pad=0,\n",
    "        half=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e56186",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt=['an oil painting of a rabbit walking in the desert']\n",
    "\n",
    "height = 640                        # default height\n",
    "width = 512                         # default width \n",
    "\n",
    "num_inference_steps = 50            # Number of denoising steps\n",
    "guidance_scale = 12.5                 # Scale for classifier-free guidance\n",
    "\n",
    "generator = torch.manual_seed(random.randint(0,250000000000))   # Seed generator to create the inital latent noise\n",
    "\n",
    "scheduler = LMSDiscreteScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", num_train_timesteps=999)\n",
    "\n",
    "batch_size = 1\n",
    "text_input = tokenizer(prompt, padding=\"max_length\", max_length=tokenizer.model_max_length, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    text_embeddings = text_encoder(text_input.input_ids.to(torch_device))[0]\n",
    "\n",
    "max_length = text_input.input_ids.shape[-1]\n",
    "uncond_input = tokenizer(\n",
    "    [\"\"] * batch_size, padding=\"max_length\", max_length=max_length, return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    uncond_embeddings = text_encoder(uncond_input.input_ids.to(torch_device))[0]\n",
    "\n",
    "text_embeddings = torch.cat([uncond_embeddings, text_embeddings])\n",
    "\n",
    "latents = torch.randn(\n",
    "  (batch_size, unet.in_channels, height // 8, width // 8),\n",
    "  generator=generator,\n",
    ")\n",
    "random_latents=latents\n",
    "latents = latents.to(torch_device)\n",
    "\n",
    "scheduler.set_timesteps(num_inference_steps)\n",
    "latents = latents * scheduler.sigmas[0]\n",
    "\n",
    "with autocast(\"cuda\"):\n",
    "  \n",
    "  for i, t in tqdm(enumerate(scheduler.timesteps)):\n",
    "    # expand the latents if we are doing classifier-free guidance to avoid doing two forward passes.\n",
    "    latent_model_input = torch.cat([latents] * 2)\n",
    "    sigma = scheduler.sigmas[i]\n",
    "    latent_model_input = latent_model_input / ((sigma**2 + 1) ** 0.5)\n",
    "\n",
    "    # predict the noise residual\n",
    "    with torch.no_grad():\n",
    "        noise_pred = unet(latent_model_input, t, encoder_hidden_states=text_embeddings)[\"sample\"]\n",
    "\n",
    "    # perform guidance\n",
    "    noise_pred_uncond ,noise_pred_text = noise_pred.chunk(2)\n",
    "    noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n",
    "    # compute the previous noisy sample x_t -> x_t-1\n",
    "    latents = scheduler.step(noise_pred, i, latents)[\"prev_sample\"]\n",
    "\n",
    "# scale and decode the image latents with vae\n",
    "latents = 1 / 0.18215 * latents\n",
    "\n",
    "with torch.no_grad():\n",
    "  image = vae.decode(latents)\n",
    "\n",
    "image = (image / 2 + 0.5).clamp(0, 1)\n",
    "img=image.detach().cpu().permute(0, 2,3, 1).squeeze(0).numpy()*255\n",
    "image = image.detach().cpu().permute(0, 2, 3, 1).numpy()\n",
    "\n",
    "images = (image * 255).round().astype(\"uint8\")\n",
    "pil_images = [Image.fromarray(image) for image in images]\n",
    "pil_images[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2deb73e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "vx=1.0+(0.00)\n",
    "vy=.00\n",
    "rl=torch.exp((random_latents*vx))+vy\n",
    "rl=torch.log(rl)\n",
    "std,mean=torch.std_mean(rl)\n",
    "rl=(rl-mean)/std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec82df8f-3f24-4c94-843c-b55f4a9235f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "rl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f64da2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt=['an oil painting of a lion walking in the desert']\n",
    "height = 640                        # default height\n",
    "width = 512                         # default width \n",
    "\n",
    "num_inference_steps = 50            # Number of denoising steps\n",
    "guidance_scale = 10                 # Scale for classifier-free guidance\n",
    "\n",
    "generator = torch.manual_seed(777)#random.randint(0,25000000))   # Seed generator to create the inital latent noise\n",
    "\n",
    "scheduler = LMSDiscreteScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", num_train_timesteps=1000)\n",
    "\n",
    "batch_size = 1\n",
    "text_input = tokenizer(prompt, padding=\"max_length\", max_length=tokenizer.model_max_length, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    text_embeddings = text_encoder(text_input.input_ids.to(torch_device))[0]\n",
    "\n",
    "max_length = text_input.input_ids.shape[-1]\n",
    "uncond_input = tokenizer(\n",
    "    [\"\"] * batch_size, padding=\"max_length\", max_length=max_length, return_tensors=\"pt\"\n",
    ")\n",
    "with torch.no_grad():\n",
    "    uncond_embeddings = text_encoder(uncond_input.input_ids.to(torch_device))[0]\n",
    "\n",
    "text_embeddings = torch.cat([uncond_embeddings, text_embeddings])\n",
    "\n",
    "latents = torch.randn(\n",
    "  (batch_size, unet.in_channels, height // 8, width // 8),\n",
    "  generator=generator,\n",
    ")\n",
    "latents=rl\n",
    "latents = latents.to(torch_device)\n",
    "\n",
    "scheduler.set_timesteps(num_inference_steps)\n",
    "latents = latents * scheduler.sigmas[0]\n",
    "\n",
    "with autocast(\"cuda\"):\n",
    "  \n",
    "  for i, t in tqdm(enumerate(scheduler.timesteps)):\n",
    "    # expand the latents if we are doing classifier-free guidance to avoid doing two forward passes.\n",
    "    latent_model_input = torch.cat([latents] * 2)\n",
    "    sigma = scheduler.sigmas[i]\n",
    "    latent_model_input = latent_model_input / ((sigma**2 + 1) ** 0.5)\n",
    "\n",
    "    # predict the noise residual\n",
    "    with torch.no_grad():\n",
    "        noise_pred = unet(latent_model_input, t, encoder_hidden_states=text_embeddings)[\"sample\"]\n",
    "\n",
    "    # perform guidance\n",
    "    noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n",
    "    noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n",
    "    # compute the previous noisy sample x_t -> x_t-1\n",
    "    latents = scheduler.step(noise_pred, i, latents)[\"prev_sample\"]\n",
    "\n",
    "# scale and decode the image latents with vae\n",
    "latents = 1 / 0.18215 * latents\n",
    "\n",
    "with torch.no_grad():\n",
    "    image = vae.decode(latents)\n",
    "\n",
    "image = (image / 2 + 0.5).clamp(0, 1)\n",
    "img=image.detach().cpu().permute(0, 2,3, 1).squeeze(0).numpy()*255\n",
    "image = image.detach().cpu().permute(0, 2, 3, 1).numpy()\n",
    "\n",
    "images = (image * 255).round().astype(\"uint8\")\n",
    "pil_images = [Image.fromarray(image) for image in images]\n",
    "pil_images[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af80ea02",
   "metadata": {},
   "outputs": [],
   "source": [
    "output, _ = upsampler.enhance(img, outscale=4)\n",
    "out_image = Image.fromarray(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5084b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a692928-5ee8-461f-ae66-4ad47fdeaa76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google_trans_new import google_translator  \n",
    "  \n",
    "translator = google_translator()  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4dc12894-d31d-44b6-913c-14f9d22d6cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "translate_text = translator.translate(\"a great person\", lang_tgt='en')  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5f97c401-4519-471c-8548-605a1b4f04b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a great person '"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16145a6f-7eef-4b17-9b92-2e0edd4b13ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
